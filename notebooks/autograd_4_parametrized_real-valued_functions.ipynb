{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "285c29cc",
   "metadata": {},
   "source": [
    "# parametrized  linear scalar function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c68059",
   "metadata": {},
   "source": [
    "Let $f: \\mathbb{R} \\times \\mathbb{R}^2 \\rightarrow \\mathbb{R}$ be a parametraized scalar function given by\n",
    "$$\n",
    "f(x; \\theta) = f(x; a, b) = a x + b. \\quad\n",
    "$$\n",
    "Its partial derivatives are\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x}f(x; a, b) = a, \\quad \n",
    "\\frac{\\partial}{\\partial a}f(x; a, b) = x, \\quad \n",
    "\\frac{\\partial}{\\partial b}f(x; a, b) = 1 .\n",
    "$$\n",
    "We want to compute the partial derivatives with respect to the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e0535b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61e36427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradient(x, grad_y):\n",
    "    if grad_y[0] == x and \\\n",
    "       grad_y[1] == 1.:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def check_gradient_vect(x, grad_y):\n",
    "    batch_size = x.shape[0]\n",
    "    if (grad_y[:, 0] == x.squeeze()).all() and \\\n",
    "       (grad_y[:, 1] == torch.ones(batch_size)).all():\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e715bc77",
   "metadata": {},
   "source": [
    "## backward() methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01023165",
   "metadata": {},
   "source": [
    "1. Multiple scalar input variables (no batch input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ced3ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, a, b):\n",
    "    return a*x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a14238",
   "metadata": {},
   "source": [
    "create torch tensors for each function variable. Impose 'requires_grad' for the parameters which we want to differentiate with respect with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a91c77b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0473], requires_grad=True) True True\n",
      "tensor([0.8933], requires_grad=True) True True\n",
      "tensor([-0.6949]) False True\n",
      "tensor([0.1655], grad_fn=<AddBackward0>) torch.FloatTensor True False\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, requires_grad=False, dtype=torch.float)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "y = linear(x, a, b)\n",
    "\n",
    "print(a, a.requires_grad, a.is_leaf)\n",
    "print(b, b.requires_grad, b.is_leaf)\n",
    "print(x, x.requires_grad, x.is_leaf)\n",
    "print(y, y.type(), y.requires_grad, y.is_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef12e0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True])\n",
      "tensor([True])\n",
      "tensor([0.])\n",
      "tensor([0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute gradients\n",
    "y.backward()\n",
    "\n",
    "# show partial derivatives\n",
    "grad_y = torch.hstack((a.grad, b.grad))\n",
    "print(a.grad == x)\n",
    "print(b.grad == 1)\n",
    "\n",
    "# reset gradients \n",
    "a.grad.zero_()\n",
    "b.grad.zero_()\n",
    "\n",
    "# show partial derivatives\n",
    "print(a.grad)\n",
    "print(b.grad)\n",
    "\n",
    "# check gradient\n",
    "check_gradient(x, grad_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a58890c",
   "metadata": {},
   "source": [
    "2. Multiple scalar input variables (batch input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "938b6b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batch size and input\n",
    "batch_size = 1000\n",
    "x_vect = torch.randn((batch_size, 1), requires_grad=False, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edfe1395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.66 s, sys: 20.1 ms, total: 1.68 s\n",
      "Wall time: 173 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# evaluate function\n",
    "y = linear(x_vect, a, b)\n",
    "\n",
    "# preallocate Jacobian matrix with respect to the parameters\n",
    "grad_y = torch.empty(batch_size, 2)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    \n",
    "    # use vector-Jacobian product\n",
    "    v = torch.eye(batch_size)[i].reshape(batch_size, 1)\n",
    "    y.backward(v, retain_graph=True)\n",
    "    \n",
    "    # save gradients\n",
    "    grad_y[i, 0] = a.grad\n",
    "    grad_y[i, 1] = b.grad\n",
    "    \n",
    "    # reset gradients\n",
    "    a.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66785d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_gradient_vect(x_vect, grad_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e00dd12",
   "metadata": {},
   "source": [
    "## grad() mehtod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04d92820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b032f09",
   "metadata": {},
   "source": [
    "1. Multiple scalar input variables (no batch input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "181214d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = linear(x, a, b)\n",
    "grad_y = grad(y, (a, b))\n",
    "check_gradient(x, grad_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180016b8",
   "metadata": {},
   "source": [
    "2. Multiple scalar input variables (batch input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00831abd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'u' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m100\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43mu\u001b[49m(x,t)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 1st derivatives\u001b[39;00m\n\u001b[1;32m      6\u001b[0m dt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(u, t)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'u' is not defined"
     ]
    }
   ],
   "source": [
    "x = torch.randn(100, requires_grad=True)\n",
    "t = torch.randn(2, requires_grad=True)\n",
    "u = u(x,t)\n",
    "\n",
    "# 1st derivatives\n",
    "dt = torch.autograd.grad(u, t)[0]\n",
    "dx = torch.autograd.grad(u, x, create_graph=True)[0]\n",
    "\n",
    "# 2nd derivatives (higher orders require `create_graph=True`)\n",
    "ddx = torch.autograd.grad(dx, x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41f4d88f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m basis_vectors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(batch_size)\n\u001b[1;32m      3\u001b[0m v \u001b[38;5;241m=\u001b[39m basis_vectors[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m \u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#grad_y = grad(y, (a, b), v, retain_graph=True)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#check_gradient(x, grad_y)\u001b[39;00m\n",
      "File \u001b[0;32m~/Dokumente/packages/pytorch/playground-pytorch/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:288\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    283\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts of the graph, please use torch.autograd.backward.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_outputs, \u001b[38;5;28mlen\u001b[39m(t_outputs))\n\u001b[0;32m--> 288\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/Dokumente/packages/pytorch/playground-pytorch/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:88\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 88\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mones_like(out, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format))\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "y = linear(x_vect, a, b)\n",
    "basis_vectors = torch.eye(batch_size)\n",
    "v = basis_vectors[0]\n",
    "grad(y, a)\n",
    "#grad_y = grad(y, (a, b), v, retain_graph=True)\n",
    "#check_gradient(x, grad_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "00af0831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# evaluate function\n",
    "#y = linear(x_vect, a, b)\n",
    "\n",
    "#part_y_x1, = grad(y, (x1,))\n",
    "\n",
    "# basis vectors\n",
    "#basis_vectors = torch.eye(batch_size)\n",
    "\n",
    "# gradient\n",
    "#grad_y = torch.stack([\n",
    "#    grad(y, (a, b), v, retain_graph=True)[0][i] \n",
    "#    for i, v in enumerate(basis_vectors.unbind())\n",
    "#])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf52ad73",
   "metadata": {},
   "source": [
    "## jacobian() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4c6dd2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.functional import jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7c6106",
   "metadata": {},
   "source": [
    "1. Compute all partial derivatives (no batch input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "96b1011b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jac_y_x, jac_y_a, jac_y_b = jacobian(linear, (x, a, b))\n",
    "grad_y = torch.hstack((jac_y_a[0], jac_y_b[0]))\n",
    "check_gradient(x, grad_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b04e4a8",
   "metadata": {},
   "source": [
    "2. Compute partial derivatives with respect to the parameters (no batch input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a8155315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jac_y_a, jac_y_b = jacobian(lambda a,b : linear(x, a, b), (a, b))\n",
    "grad_y = torch.hstack((jac_y_a[0], jac_y_b[0]))\n",
    "check_gradient(x, grad_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245bd0bf",
   "metadata": {},
   "source": [
    "3. Compute all partial derivatives (batch input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "aa3c1d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 1])\n",
      "torch.Size([1000, 1, 1])\n",
      "CPU times: user 36.2 ms, sys: 19 µs, total: 36.2 ms\n",
      "Wall time: 35.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# compute gradients\n",
    "jac_y = jacobian(linear, (x_vect, a, b))\n",
    "jac_y_x, jac_y_a, jac_y_b = jac_y\n",
    "\n",
    "# show partial derivatives\n",
    "print(jac_y_a.shape)\n",
    "print(jac_y_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c6d7239a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_gradient_vect(x_vect, torch.hstack((jac_y_a.squeeze(dim=2), jac_y_b.squeeze(dim=2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84684d35",
   "metadata": {},
   "source": [
    "4. Compute just the partial derivatives with respect to the parameters (batch input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "62ecf88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 1])\n",
      "torch.Size([1000, 1, 1])\n",
      "CPU times: user 31 ms, sys: 0 ns, total: 31 ms\n",
      "Wall time: 30.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# compute gradients\n",
    "jac_y = jacobian(lambda a,b : linear(x_vect, a, b), (a, b))\n",
    "jac_y_a, jac_y_b = jac_y\n",
    "\n",
    "# show partial derivatives\n",
    "print(jac_y_a.shape)\n",
    "print(jac_y_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3ccee99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_gradient_vect(x_vect, torch.hstack((jac_y_a.squeeze(dim=2), jac_y_b.squeeze(dim=2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44d86f9",
   "metadata": {},
   "source": [
    "# parametrized  linear scalar function (using the nn Module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0344019",
   "metadata": {},
   "source": [
    "### no batch input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a9ceec57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 2\n",
      "Parameter containing:\n",
      "tensor([[-0.9509]], requires_grad=True) True True\n",
      "Parameter containing:\n",
      "tensor([-0.1825], requires_grad=True) True True\n",
      "tensor([-0.3124]) False True\n",
      "tensor([0.1146], grad_fn=<AddBackward0>) torch.FloatTensor True False\n",
      "tensor([[True]])\n",
      "tensor([True])\n"
     ]
    }
   ],
   "source": [
    "# create linear model\n",
    "d_in, d_out = 1, 1\n",
    "model = nn.Linear(d_in, d_out, bias=True)\n",
    "parameters = list(model.parameters())\n",
    "print(type(parameters), len(parameters))\n",
    "\n",
    "# get parameters\n",
    "a = model._parameters['weight']\n",
    "b = model._parameters['bias']\n",
    "\n",
    "# evaluate model at x\n",
    "x = torch.randn(d_in, requires_grad=False, dtype=torch.float)\n",
    "y = model(x)\n",
    "\n",
    "print(a, a.requires_grad, a.is_leaf)\n",
    "print(b, b.requires_grad, b.is_leaf)\n",
    "print(x, x.requires_grad, x.is_leaf)\n",
    "print(y, y.type(), y.requires_grad, y.is_leaf)\n",
    "\n",
    "# run .backward() + partial derivatives\n",
    "y.backward()\n",
    "print(a.grad == x)\n",
    "print(b.grad == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba2cb5",
   "metadata": {},
   "source": [
    "## batch input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ead039",
   "metadata": {},
   "source": [
    "### using backwards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "51a7b2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor(False)\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "# define batch size and input\n",
    "batch_size = 10\n",
    "x = torch.randn((batch_size, d_in), requires_grad=False, dtype=torch.float)\n",
    "\n",
    "# evaluate model\n",
    "y = model(x)\n",
    "\n",
    "# preallocate Jacobian matrix with respect to the coefficients\n",
    "jac_y = torch.empty(batch_size, 2)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    \n",
    "    # use vector-Jacobian product\n",
    "    v = torch.eye(batch_size)[i].reshape(batch_size, 1)\n",
    "    y.backward(v, retain_graph=True)\n",
    "    \n",
    "    # save gradients\n",
    "    jac_y[i, 0] = a.grad\n",
    "    jac_y[i, 1] = b.grad\n",
    "    \n",
    "    # reset gradients\n",
    "    a.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "# show Jacobian\n",
    "print(jac_y)\n",
    "print((jac_y[:, 0] == x[:, 0]).all())\n",
    "print((jac_y[:, 1] == torch.ones(batch_size)).all())\n",
    "#print(jac_y[:, 0], x[:, 0])\n",
    "#print(jac_y[:, 1], torch.ones(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1d8bd8",
   "metadata": {},
   "source": [
    "### using jacobian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0606ddf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 1, 1])\n",
      "torch.Size([10, 1, 1])\n",
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# define batch size and input\n",
    "batch_size = 10\n",
    "x = torch.randn((batch_size, d_in), requires_grad=False, dtype=torch.float)\n",
    "\n",
    "def partial_model(a, b):\n",
    "    model._parameters['weight'] = a\n",
    "    model._parameters['bias'] = b\n",
    "    return model(x)\n",
    "\n",
    "# compute gradients\n",
    "jac_y = jacobian(partial_model, (a, b))\n",
    "jac_y_a, jac_y_b = jac_y\n",
    "\n",
    "# show partial derivatives\n",
    "print(jac_y_a.shape)\n",
    "print(jac_y_b.shape)\n",
    "print((jac_y_a[:, 0, 0, 0] == x[:, 0]).all())\n",
    "print((jac_y_b[:, 0, 0] == torch.ones(batch_size)).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c2656a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 1, 1])\n",
      "torch.Size([10, 1, 1])\n",
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# define batch size and input\n",
    "batch_size = 10\n",
    "x = torch.randn((batch_size, d_in), requires_grad=False, dtype=torch.float)\n",
    "\n",
    "# compute gradients\n",
    "jac_y = jacobian(linear, (x, a, b))\n",
    "jac_y_x, jac_y_a, jac_y_b = jac_y\n",
    "\n",
    "# show partial derivatives\n",
    "print(jac_y_a.shape)\n",
    "print(jac_y_b.shape)\n",
    "print((jac_y_a[:, 0, 0, 0] == x[:, 0]).all())\n",
    "print((jac_y_b[:, 0, 0] == torch.ones(batch_size)).all())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1720ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cac256f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
